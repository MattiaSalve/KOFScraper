from pathlib import Path
from scrapy.exporters import CsvItemExporter
from ARGUS.items import DualExporter
import ARGUS.data as data
from bin.durations import save_spider_duration

import time
import datetime
import re
import gzip
import os


class DualPipeline(object):
    def open_spider(self, spider):
        url_chunk_path = Path(spider.url_chunk).resolve()
        chunk_id = url_chunk_path.stem.split("_")[-1]

        # Use the parent directory of the chunk file to save results
        output_dir = url_chunk_path.parent
        output_dir.mkdir(parents=True, exist_ok=True)

        output_path = output_dir / f"ARGUS_chunk_{chunk_id}.csv.gz"
        self.f = gzip.open(output_path, mode="wb")

        self.exporter = CsvItemExporter(
            self.f, encoding="utf-8", delimiter="\t", include_headers_line=True
        )
        self.exporter.fields_to_export = [
            "ID",
            "dl_rank",
            "dl_slot",
            "alias",
            "error",
            "redirect",
            "start_page",
            "title",
            "keywords",
            "description",
            "language",
            "text",
            "links",
            "timestamp",
            "url",
        ]

        raw_path = output_dir / f"ARGUS_chunk_{chunk_id}_raw.csv.gz"
        self.raw_f = gzip.open(raw_path, mode="wb")
        self.raw_exporter = CsvItemExporter(
            self.raw_f, encoding="utf-8", delimiter="\t", include_headers_line=True
        )
        self.raw_exporter.fields_to_export = [
            "ID",
            "dl_rank",
            "dl_slot",
            "alias",
            "error",
            "redirect",
            "start_page",
            "html_raw",
            "url",
        ]

        self.exporter.start_exporting()
        self.raw_exporter.start_exporting()

        self._chunk_id = chunk_id
        self._output_dir = output_dir

    def close_spider(self, spider):
        try:
            if hasattr(self, "exporter"):
                self.exporter.finish_exporting()
            if hasattr(self, "raw_exporter"):
                self.raw_exporter.finish_exporting()
            if hasattr(self, "f"):
                self.f.close()
            if hasattr(self, "raw_f"):
                self.raw_f.close()

            stats = self.crawler.stats
            start_time = stats.get_value("start_time")
            end_time = stats.get_value("finish_time")
            duration = (end_time - start_time).total_seconds()
            print(f"Spider duration: {duration} seconds")
            save_spider_duration(f"spider_{self._chunk_id}", duration)

            print("Closing spider...")
            url_chunk_path = Path(spider.url_chunk).resolve()
            chunk_id = url_chunk_path.stem.split("_")[-1]

            # Use the parent directory of the chunk file to save results
            output_dir = url_chunk_path.parent
            output_dir.mkdir(parents=True, exist_ok=True)

            output_path = output_dir / f"ARGUS_chunk_{chunk_id}.csv.gz"

            df = data.load_data(output_path)

        except Exception as e:
            print(f"[ERROR] Post-analysis failed: {e}")
        # if hasattr(self, "exporter"):
        #     self.exporter.finish_exporting()
        # if hasattr(self, "f"):
        #     self.f.close()

    def process_item(self, item, spider):
        tag_pattern = re.compile(r"(\[->.+?<-\] ?)+?")
        scraped_text = item["scraped_text"]
        c = 0

        for webpage in scraped_text:
            webpage_exporter = DualExporter()
            webpage_exporter["dl_slot"] = item["dl_slot"][0]
            webpage_exporter["start_page"] = item["start_page"][0]
            webpage_exporter["url"] = item["scraped_urls"][c]
            webpage_exporter["redirect"] = item["redirect"][0]
            webpage_exporter["error"] = item["error"]
            webpage_exporter["ID"] = item["ID"][0]
            webpage_exporter["alias"] = item["alias"][0]
            webpage_exporter["html_raw"] = item.get("html_raw", [""])[0]

            title = item["title"][c]
            description = item["description"][c]
            keywords = item["keywords"][c]
            language = item["language"][c]

            webpage_exporter["title"] = title.strip()
            webpage_exporter["description"] = description.strip()
            webpage_exporter["keywords"] = keywords.strip()
            webpage_exporter["language"] = language.strip()

            webpage_exporter["links"] = list({link for link in item["links"] if link})

            webpage_text = ""
            for tagchunk in webpage:
                text_piece = " ".join(tagchunk[-1][0].split()).strip()
                if not text_piece:
                    continue

                splitted = re.split(tag_pattern, text_piece)
                text_piece = ""
                for i, elem in enumerate(splitted):
                    if i % 2 == 0 and elem.strip().strip('"'):
                        text_piece += splitted[i - 1] + elem

                if text_piece:
                    webpage_text += ". " + text_piece

            webpage_exporter["text"] = (
                webpage_text[2:] if webpage_text.startswith(". ") else webpage_text
            )
            webpage_exporter["timestamp"] = datetime.datetime.fromtimestamp(
                time.time()
            ).strftime("%c")
            webpage_exporter["dl_rank"] = c

            self.exporter.export_item(webpage_exporter)

            raw_record = {
                "ID": item["ID"][0],
                "dl_rank": c,
                "url": url,
                "timestamp": timestamp,
                "html_raw": item.get("html_raw", [""])[0],
            }
            # self.exporter.export_item(raw_record)
            self.raw_exporter.export_item(raw_record)
            c += 1

        return item
